{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalB_release beltz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjnUTgMYK-b9"
      },
      "source": [
        "# Problem 1 (12 points)\n",
        "\n",
        "Recall the word ladder puzzle from Homework 1, in which vowel changes cost twice those of consonant changes. You implemented best-first search as a general procedure for solving this problem. Suppose now that we do not maintain a ```reached``` table and that children of expanded nodes are always added to the frontier.\n",
        "\n",
        "1. Explain how this change affects the property of completeness. Remember that it is possible for a problem to have no solution. Consider each of the following algorithms: depth-first search, breadth-first search, uniform-cost search, and A* search with the simple Hamming distance heuristic.\n",
        "\n",
        "2. Suppose that multiple solutions exist for a given problem. Without a reached table, which of the above four algorithms are guaranteed to return an optimal solution, and which are not? Explain for each of them.\n",
        "\n",
        "3. Explain whether the \"simple\" Hamming distance and the Hamming distance with vowels heuristics are still admissible when A* is implemented without a reached table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwHNvjNMaM1"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. The reached table limits completeness, since it limits finding alternate solutions that use the same word. For DFS, the infinite nature of the problem means it will never be complete, but the reached table makes it less complete. For BFS, removing the reached table makes the search complete since it systematically goes throught each node in the tree. For UCS, removing the reached table makes it complete since it visits every node systematically. For an A* search with a Hamming distance heuristic, removing the reached table makes it complete since it also would search through all the nodes in the tree in a systematic way similar to UCS.\n",
        "\n",
        "2. DFS is not guaranteed to return the optimal solution since it goes down one branch and returns the first solution seen. Since it only minimizes distance to the root node, BFS is only guaranteed to return the optimal solution if costs are uniform, which they are not in this case, so BFS is not guaranteed to return the optimal solution. UCS is is guaranteed to return the optimal solution since it checks nodes in order of minimal cost and returns the first one seen. A* search is only optimal if the heuristic function is admissable. Since the Hamming distance heuristic is admissable, A* is optimal.\n",
        "\n",
        "3. Yes, they are still admissable because the reached table does not affect admissibility. Hamming distance is admissable since the total cost will always be greater than or equal to the number of indices where the corresponding letters are different. The Hamming distance with vowels is even more accurate, since it always is equal to the true cost. The second heuristic dominates the first since its value is always greater than or equal to the firsts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYyNrCNC7kJ"
      },
      "source": [
        "# Problem 2 (16 points)\r\n",
        "\r\n",
        "We are computing the value $v_0$ of a chance node $C_0$ in a game tree. $C_0$ has children $C_1, C_2, ..., C_n$, occurring with probability $p_1, p_2, ..., p_n$, respectively. We have already retrieved the values of the first $k$ children: $v_1, ..., v_k$. We have not yet seen the values of the remaining children nodes. Give an expression for the maximum possible value $v_0$, if the maximum possible value for any child node is $v_{\\text{max}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5uqMxKoD-HA"
      },
      "source": [
        "ENTER YOUR EXPRESSION HERE\r\n",
        "\r\n",
        "(v1 * p1) + (v2 * p2) + ... + (vk * pk) + (vmax  * pk+1) + (vmax * pk+2) + ... + (vmax * pn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osyq0--xMqjV"
      },
      "source": [
        "X and O are playing 3x3 tic-tac-toe. Instead of playing optimally, O now plays randomly, choosing its move according to known probabilities each turn. X's strategy is implemented in an ```X_value``` function (signature shown below). You also have access to functions returning O's ```actions``` in a given state and the ```result``` of taking an action from a given state.\n",
        "\n",
        "```\n",
        "def X_value(state):\n",
        "  INPUTS: A game state\n",
        "  OUTPUT: Value of state assuming it is X's turn to move\n",
        "\n",
        "def actions(state):\n",
        "  INPUT: A game state during O's turn\n",
        "  OUTPUTS: List of (action, probability) tuples\n",
        "\n",
        "def result(state, action):\n",
        "  INPUTS: A game state and action\n",
        "  OUTPUT: New game state as a result of O taking action from state\n",
        "```\n",
        "\n",
        "Calling the above functions as necessary, complete the ```O_value``` function below, which returns the value of the given game state assuming it is O's turn to move. As the function loops through the children states to compute the expected value, it should also prune when possible. ```O_value``` should stop retrieving the values of its remaining children nodes as soon as it recognizes that it is no longer possible to return a value greater than ```alpha``` (it can simply return the value computed so far).\n",
        "\n",
        "As a hint, you should be using the expression you derived above. You may assume that ```V_MAX``` is a known upper bound on state utility values and accessible in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AToluZq8PPHR"
      },
      "source": [
        "def O_value(state, alpha):\n",
        "  isTerminal, score = terminal(state)\n",
        "  if isTerminal:\n",
        "    return score\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  expected_val = 0\n",
        "  total_prob = 0\n",
        "\n",
        "  o_actions = actions(state)\n",
        "\n",
        "  for tup in o_actions:\n",
        "    total_prob += tup[1]\n",
        "\n",
        "    expected_val += tup[1] * X_value( result( state, tup[0] ) )\n",
        "\n",
        "    if expected_val + (1-total_prob) * V_MAX <= alpha:\n",
        "      break\n",
        "\n",
        "  return expected_val\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF6oNrSzsbjE"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pYqZlMfs-Sl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ca8e7f-fe11-4962-e8e5-4f30febdd2f8"
      },
      "source": [
        "def terminal(s):\n",
        "  return False, None\n",
        "def actions(s):\n",
        "  return [('a1',0.5), ('a2',0.5)]\n",
        "def result(s,a):\n",
        "  if a == 'a1': return 's1'\n",
        "  elif a == 'a2': return 's2'\n",
        "def X_value(s):\n",
        "  if s == 's1': return 1\n",
        "  elif s == 's2': return 1\n",
        "\n",
        "V_MAX = 1\n",
        "print(O_value('s0',1))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhuewOeyQp4K"
      },
      "source": [
        "# Problem 3 (20 points)\n",
        "\n",
        "Recall that $TD(0)$ solves the prediction problem of evaluating a fixed policy $\\pi$ using temporal-difference updates to estimated state values. The update to the value $V^\\pi(s)$ is given by\n",
        "\n",
        "$$ V^\\pi(s) \\leftarrow V^\\pi(s) + \\alpha (r + \\gamma V^\\pi(s') - V^\\pi(s)), $$\n",
        "\n",
        "where $r$ is the observed reward, $s'$ is the successor state, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor.\n",
        "\n",
        "Suppose that our agent has traversed a sequence of states $s_1, s_2, ..., s_n$ (following $\\pi$) and observed the corresponding rewards $r_1, r_2, ..., r_{n-1}$. All distinct states in the problem have been encountered at least once prior to $s_n$. The underlying transition and reward functions map $(s,a,s')$ inputs to fixed probabilities and real values, respectively. \n",
        "\n",
        "1. Provide a set of equations in terms of the observed states, rewards, and problem parameters that, if solvable, produces the values to all states $V^\\pi(s)$ such that all TD updates using the observed state-reward sequences are zero. It is acceptable for your set to contain possibly redundant equations.\n",
        "\n",
        "2. How many unknowns are there in your equations? Explain whether it is possible to have more *unique* equations than unknowns and why that may occur.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEXBTZgQB_gJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\r\n",
        "\r\n",
        "1. 0 = r1 + gamma * V(s2) - V(s1),\r\n",
        "0 = r2 + gamma * V(s3) - V(s2),\r\n",
        "... ,\r\n",
        "0 = rn-1 + gamma * V(sn) - V(sn-1)\r\n",
        "\r\n",
        "\r\n",
        "2. There are n unknowns in the equations, each being a value we are trying to solve. It is possible to have more unknowns than equations since each equation represents a transition between states which is fewer than the actual number of states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTKfM0MVLFWL"
      },
      "source": [
        "As with policy evaluation using a known model, an alternative to solving the equations that you wrote is dynamic programming, this time using the TD update rather than Bellman update. Starting with an initialized dictionary of values ```V``` of the form ```{state:value}```, this scheme should sweep over the lists of observed ```states``` and ```rewards``` sequences, performing a TD update to  ```V``` for each state-reward pair seen. This sweep should then be repeated until the maximum absolute change for a value is smaller than the provided ```threshold```. (You may assume that the provided inputs will produce a solution that converges.)\r\n",
        "\r\n",
        "Implement the described algorithm in ```TD0``` below. In addition to the described inputs, note that we also have the ```alpha``` and ```gamma``` parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPsJniMuDfIT"
      },
      "source": [
        "def TD0(V, states, rewards, alpha, gamma, threshold):\n",
        "  max_diff = float(\"inf\")\n",
        "  while max_diff >= threshold:\n",
        "    # YOUR CODE HERE \n",
        "    temp_max = -1\n",
        "\n",
        "    for x in range(len(rewards)):\n",
        "      error = rewards[x] + gamma * V[states[x+1]] - V[states[x]]\n",
        "      V[states[x]] = V[states[x]] + alpha * error\n",
        "\n",
        "      if error > temp_max:\n",
        "        temp_max = error\n",
        "\n",
        "    max_diff = temp_max\n",
        "\n",
        "  return V"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CnvxuufwsTS"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1isalpwsTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0a9bb8-e5cc-4016-907e-46f697cdef0f"
      },
      "source": [
        "states = ['s1', 's2', 's1']\n",
        "rewards = [1, 1]\n",
        "alpha = 0.5\n",
        "gamma = 0.8\n",
        "threshold = 1e-6\n",
        "\n",
        "V = {'s1':0, 's2':0}\n",
        "print(TD0(V, states, rewards, alpha, gamma, threshold))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'s1': 4.999996728139351, 's2': 4.9999969413111405}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuCoInshppF6"
      },
      "source": [
        "# Problem 4 (16 points)\n",
        "\n",
        "We have a Naive Bayes model with class variable $Y$ and feature variables $F_1, ..., F_n$. Suppose we observe feature $F_e = f_e$. \n",
        "\n",
        "1.  Give an expression for $\\Pr(F_q \\mid f_e)$, the distribution of the query feature $F_q$ given the evidence feature $f_e$. You may also provide your answer in the form of an unnormalized distribution. All quantities in your expression should be contained in the set of Naive Bayes parameters.\n",
        "\n",
        "2.  Briefly describe how you would estimate $\\Pr(F_q \\mid f_e)$ using likelihood weighting instead. What variables should be sampled, what order are they sampled in, and what sampling distributions are used? Also be sure to describe how the desired distribution is obtained from samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULf95khAEnN"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\r\n",
        "\r\n",
        "1.  Sum_y ( Pr(fe | y) * Pr(y) * Pr(Fq | y) )\r\n",
        "\r\n",
        "2.  I would sample Y and Fq while fixing evidence fe. I would sample in topological order. The sampling distributions used are the known distribution which we sample from and the unknown target distribution which we are trying to find out. The desired distribution is obtained from the samples by weighting each sample based on the probability of evidence given its parents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObUO85S9Acc-"
      },
      "source": [
        "Use the expression that you came up with in 1. to implement ```feature_likelihood``` below. The function inputs are as follows:\r\n",
        "*   ```prior``` is a 1D numpy array containing the distribution $\\Pr(Y)$.\r\n",
        "*   ```fe_given_y``` is a 1D numpy array containing the probabilities $\\Pr(f_e \\mid Y)$.\r\n",
        "*   ```fq_given_y``` is a 2D numpy array, in which ```fq_given_y[i,j]``` is the probability $\\Pr(F_q = i \\mid Y = j)$.\r\n",
        "\r\n",
        "Make sure that your outputs are properly normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmispu48d1m"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def feature_likelihood(prior, fe_given_y, fq_given_y):\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  dists = list()\n",
        "\n",
        "  for i in range(len(fq_given_y)):\n",
        "    dists.append(prior * fe_given_y * fq_given_y[i])\n",
        "\n",
        "  for x in range(len(dists)):\n",
        "    sum = np.sum(dists[x])\n",
        "    dists[x] = dists[x] / sum\n",
        "\n",
        "  return dists\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KFErZRABW2"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJoAKIgk-39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bf63ff-53d3-48e7-fd4a-df17e4d9e0b7"
      },
      "source": [
        "prior = np.array([0.5, 0.5])\n",
        "fe_given_y = np.array([0.7, 0.6])\n",
        "fq_given_y = np.array([[0.3, 0.6], [0, 0.4], [0.7, 0]])\n",
        "\n",
        "print(feature_likelihood(prior, fe_given_y, fq_given_y))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.36842105, 0.63157895]), array([0., 1.]), array([1., 0.])]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}