{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Michael Beltz - hw2b_release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAjd_8vxaZPf"
      },
      "source": [
        "# Problem 1: Crossword CSP (30 points)\n",
        "\n",
        "Run or download the latest version (4.6.1) of the \"Consistency based CSP solver\" applet from [AIspace](http://www.aispace.org/downloads.shtml). Then navigate to \"File\" at the top menu, followed by \"Load Sample CSP\", and select \"Crossword Problem 2\". This CSP represents the simple crossword shown below. The goal of this puzzle is to fill in the grid such that each row and column of letters is an English word. The CSP is thus naturally formulated with each row or column as a variable, each domain as the set of possible words that may be filled in, and each constraint specifying letters that must match.\n",
        "\n",
        "![crossword.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDLkBncejJY"
      },
      "source": [
        "1. Give an explicit form of the constraint between \"1-across\" and \"1-down\". You can click on each variable to see its full domain after clicking on the \"Solve\" tab near the upper left.\n",
        "\n",
        "2. Suppose we are doing backtracking search. Which variable(s) should we assign first based on the degree heuristic? What about the MRV heuristic? Assume that we have not yet made any assignments for either case.\n",
        "\n",
        "3. We start doing backtracking search and assign \"ginger\" to \"2-down\". Which domain reductions take place as a result of **forward checking**? Should we proceed with any further assignments?\n",
        "\n",
        "4. We start doing backtracking search and assign \"big\" to \"1-across\". Which domain reductions take place as a result of enforcing **arc consistency** on the entire CSP? Should we proceed with any further assignments?\n",
        "\n",
        "5. Reset the CSP to its initial state and perform full arc consistency without any initial assignments (click the button \"Auto Arc-Consistency\"). What happens when you make any assignment and then run arc consistency once more? How many solutions are there to this crossword puzzle?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYWMB44gg3qM"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. C = {(oneacross, onedown) âˆˆ {(big, book), (big,buys), (bus, book), (bus, buys), (has, hold)}} \n",
        "2. Degree: twodown MRV: twodown\n",
        "3. oneacross is reduced to {big}, threeacross is reduced to {land}, and fouracross is reduced to nothing, so we should not proceed with further assignments since a domain is empty. The assignment failed. We should not proceed with further assignments.\n",
        "4. onedown is reduced to {book, buys} and twodown is reduced to {ginger}, then threeacross is reduced to {year}, which further reduced onedown to {buys} to keep arc consistency. However, this conflicts with twodown, so this assignment was not arc consistant and not valid. We should not proceed with further assignments.\n",
        "5.  When I make an assignment and run arc consistency, it finds a solution by reducing the remaining domains. There are 2 solutions to this crossword.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZmqV-oVjUD"
      },
      "source": [
        "# *(m,n,k)*-Game\n",
        "\n",
        "Here you will design and play with a simple agent for a [*m,n,k*-game](https://en.wikipedia.org/wiki/M,n,k-game). This is a generalization of tic-tac-toe to a *m* x *n* board, with the goal of getting *k* in a row. As in tic-tac-toe, there are two players, X and O, and they take turns marking cells on the board. Here are our rules:\n",
        "*   This is a zero-sum game; a win for X is equally a loss for O and vice versa. We take X to be the maximizing player and O to be the minimizing player.\n",
        "*   There are no \"draws\" in this game. O wins as long as X does not get *k* in a row; thus, draws in regular tic-tac-toe (no empty spaces left) are considered wins for O and losses for X.\n",
        "*   Unless otherwise specified, X always goes first.\n",
        "\n",
        "You'll find the game-specific functions ```actions```, ```result```, and ```terminal``` used by minimax and alpha-beta search below. The last one also calls the utility functions ```k_in_row``` and ```sequences```. Here are the details...\n",
        "*   States are represented as 2d numpy arrays. Elements may be 'X', 'O', or '.' for blanks.\n",
        "*   Taking an action means picking a blank space and marking it. ```actions``` returns all possible actions, or equivalently, all indices of blanks.\n",
        "*   ```result``` creates and returns a new state with the indicated space filled in by the given player (either 'X' or 'O').\n",
        "* ```terminal``` serves as both a terminal test and utility function to save some work. First return argument is True (if someone has won) or False; second argument is utility if the state is terminal or ```None``` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7aXnCtqIKU2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def actions(state):\n",
        "  # Returns indices of all blank spaces on board\n",
        "  return [i for i,s in np.ndenumerate(state) if s=='.']\n",
        "\n",
        "def result(state, player, action):\n",
        "  # Returns a new state (deepcopied) with action space taken by player\n",
        "  new_state = state.copy()\n",
        "  new_state[action] = player\n",
        "  return new_state\n",
        "\n",
        "def terminal(state, k):\n",
        "  # Test whether state is a terminal or not; also return game score if yes\n",
        "  X_indices = [i for i,s in np.ndenumerate(state) if s=='X']\n",
        "  if k_in_row(X_indices, k): \n",
        "    return True, 1\n",
        "  O_indices = [i for i,s in np.ndenumerate(state) if s=='O']\n",
        "  blanks = np.count_nonzero(state == '.')\n",
        "  if k_in_row(O_indices, k) or blanks == 0: \n",
        "    return True, -1\n",
        "  return False, None\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Utility functions used by terminal (above)\n",
        "\n",
        "def k_in_row(indices, k):\n",
        "  # Test whether there are k consecutive indices in a row in the given list\n",
        "  index_set = set(indices)\n",
        "  for i in indices:\n",
        "    for seq in sequences(i, k):\n",
        "      if seq.issubset(index_set):\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "def sequences(i, k):\n",
        "  # Return 4 sets of k indices in the \"rows\" starting from index i\n",
        "  across = set([(i[0], i[1]+j) for j in range(k)])\n",
        "  down = set([(i[0]+j, i[1]) for j in range(k)])\n",
        "  diagdown = set([(i[0]+j, i[1]+j) for j in range(k)])\n",
        "  diagup = set([(i[0]+j, i[1]-j) for j in range(k)])\n",
        "  return across, down, diagdown, diagup"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJtXYwh-gXy9"
      },
      "source": [
        "## Coding 1 (20 points)\n",
        "\n",
        "We will now design an alpha-beta search agent to play this game. The skeleton functions below follow the pseudocode provided in the lecture and textbook. The inclusion of *k* as an argument allows it to be passed into ```terminal```. Write the main loops of the ```max_value``` and ```min_value``` functions. Remember that the ```result``` function takes in a player argument, and that X is MAX while O is MIN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bujrabhUJIiz"
      },
      "source": [
        "def alpha_beta_search(state, player, k):\n",
        "  # Initialize a game tree search for (m,n,k) game\n",
        "  # X is maximizing player, O is minimizing player\n",
        "  if player == 'X':\n",
        "    value, move = max_value(state, -float(\"inf\"), float(\"inf\"), k)\n",
        "  else:\n",
        "    value, move = min_value(state, -float(\"inf\"), float(\"inf\"), k)\n",
        "  return value, move\n",
        "\n",
        "def max_value(state, alpha, beta, k):\n",
        "  isTerminal, score = terminal(state, k)\n",
        "  if isTerminal:\n",
        "    return score, None\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  move = None\n",
        "  maxEval = -float(\"inf\")\n",
        "  \n",
        "  for action in actions(state):\n",
        "    # get new state from action\n",
        "    new_state = result(state, 'X', action)\n",
        "\n",
        "    # get the value and move from that state from the min_value function\n",
        "    eval, eval_move = min_value(new_state, alpha, beta, k)\n",
        "\n",
        "    # if the eval is the biggest yet, it is optimal to follow it, so we save that val and move\n",
        "    if eval > maxEval:\n",
        "      maxEval = eval\n",
        "      move = action\n",
        "\n",
        "    # we see if alpha is greater than beta to prune\n",
        "    alpha = max(alpha, eval)\n",
        "    if beta <= alpha:\n",
        "      break\n",
        "\n",
        "  return maxEval, move\n",
        "\n",
        "\n",
        "\n",
        "def min_value(state, alpha, beta, k):\n",
        "  isTerminal, score = terminal(state, k)\n",
        "  if isTerminal:\n",
        "    return score, None\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  move = None\n",
        "  minEval = float(\"inf\")\n",
        "\n",
        "  for action in actions(state):\n",
        "    # get new state from action\n",
        "    new_state = result(state, 'O', action)\n",
        "\n",
        "    # get the value and move from that state from the min_value function\n",
        "    eval, eval_move = max_value(new_state, alpha, beta, k)\n",
        "\n",
        "    # if the eval is the smallest yet, it is optimal to follow it, so we save that val and move\n",
        "    if eval < minEval:\n",
        "      minEval = eval\n",
        "      move = action\n",
        "\n",
        "    # update beta\n",
        "    beta = min(beta, eval)\n",
        "  \n",
        "  return minEval, move\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG70uDaqiuN9"
      },
      "source": [
        "You can test the functions above by either finding the values of terminal states or states close to terminal states, for which you know the winner with certainty. (Remember that states are 2d numpy arrays.) For example, in the (3,3,3) game, ```max_value``` of a state with two X's and a blank in a row should certainly return a move leading to a winning state for X. Same goes for ```min_value``` and O.\n",
        "\n",
        "Once you're happy with your agent implementation, let's test it out in a full game. The following ```game_loop``` takes in an initial state, the value of *k*, the search algorithm to use (which for now is just ```alpha_beta_search```) and any additional parameters for the search algorithm (none for now). \n",
        "\n",
        "When it runs, ```game_loop``` simply goes back and forth between the two players. Each player plans its next move using ```search```, the move is executed, the state is updated, and the opposing player's turn is taken. This ends when we hit a terminal state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_h1B0zOPUZU"
      },
      "source": [
        "def game_loop(state, k, search, X_params=[], O_params=[]):\n",
        "  # Play a (m,n,k) game using provided search function and parameters\n",
        "  player = 'X'\n",
        "  isTerminal = False\n",
        "  while not isTerminal:\n",
        "    if player == 'X':\n",
        "      value, move = search(state, player, k, *X_params)\n",
        "      state = result(state, player, move)\n",
        "      player = 'O'\n",
        "    else:\n",
        "      value, move = search(state, player, k, *O_params)\n",
        "      state = result(state, player, move)\n",
        "      player = 'X'\n",
        "    print(np.matrix(state), \"\\n\")\n",
        "    isTerminal, _ = terminal(state, k)\n",
        "\n",
        "  if value > 0: print(\"X wins!\")\n",
        "  elif value < 0: print(\"O wins!\")\n",
        "  else: print(\"Draw!\")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwc2sJdInh0z"
      },
      "source": [
        "Let's have our agent play against itself! Run the code below to play standard tic-tac-toe. Remember that both players play optimally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHZOccQoZjjh",
        "outputId": "2ed79eac-b241-49de-9f89-9ec7cb52f5a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "m, n, k = 3, 3, 3 \n",
        "print((m,n,k))\n",
        "initial = np.full((m,n), '.')\n",
        "game_loop(initial, k, alpha_beta_search)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 3, 3)\n",
            "[['X' '.' '.']\n",
            " ['.' '.' '.']\n",
            " ['.' '.' '.']] \n",
            "\n",
            "[['X' '.' '.']\n",
            " ['.' 'O' '.']\n",
            " ['.' '.' '.']] \n",
            "\n",
            "[['X' 'X' '.']\n",
            " ['.' 'O' '.']\n",
            " ['.' '.' '.']] \n",
            "\n",
            "[['X' 'X' 'O']\n",
            " ['.' 'O' '.']\n",
            " ['.' '.' '.']] \n",
            "\n",
            "[['X' 'X' 'O']\n",
            " ['X' 'O' '.']\n",
            " ['.' '.' '.']] \n",
            "\n",
            "[['X' 'X' 'O']\n",
            " ['X' 'O' '.']\n",
            " ['O' '.' '.']] \n",
            "\n",
            "O wins!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syPMAQhsnaiM"
      },
      "source": [
        "## Response 1 (5 points)\n",
        "\n",
        "What was the result of the game? Assuming both players play optimally, can we ever have a different player win? What if we change the starting player, or choose a different move wherever minimax returns multiple equally good moves? Explain why it is or is not possible for a different final result to arise with these changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njmrJiJaoaoI"
      },
      "source": [
        "\n",
        "The result was that player O wins every time. Since both players play optimally, O will always win since they win the draws. \n",
        "\n",
        "Even if we change the starting player or return a different equally optimal move, O will always win. \n",
        "\n",
        "The starting player doesn't matter because no matter who starts in tic tac toe, an optimal player can always lead it to a draw. Since O wins draws, O wins no matter who starts. \n",
        "\n",
        "Returning a different equally optimal move will also not change the outcome since the search checks all possible outcones. Therefore any more that is just is optimal also ends in O winning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcwt_bFoerK"
      },
      "source": [
        "Our code is suitable for any values of *m,n,k*. Below are three game instances to try out (you could also try more, although any board larger than 4x4 will probably take a long time to finish). Run each scenario, think about the results, and answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGE1wSeOP27A",
        "outputId": "d272b57c-dbea-4898-9a12-e2c49b1edd3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#m, n, k = 2, 5, 3\n",
        "#m, n, k  = 3, 4, 3\n",
        "m, n, k = 3, 4, 4\n",
        "print((m,n,k))\n",
        "initial = np.full((m,n), '.')\n",
        "game_loop(initial, k, alpha_beta_search)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 4, 4)\n",
            "[['X' '.' '.' '.']\n",
            " ['.' '.' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' '.' '.']\n",
            " ['.' '.' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' '.']\n",
            " ['.' '.' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['.' '.' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' '.' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' '.' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' '.']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']\n",
            " ['.' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']\n",
            " ['X' '.' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']\n",
            " ['X' 'O' '.' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' '.']] \n",
            "\n",
            "[['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']\n",
            " ['X' 'O' 'X' 'O']] \n",
            "\n",
            "O wins!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nExuZI28rqKY"
      },
      "source": [
        "## Response 2 (10 points)\n",
        "1.  Which of the three scenarios is X able to win? Explain why it appears as if O is not even trying to prevent X from winning at any point in this game.\n",
        "2.  What is happening in the last scenario (3,4,4)? Why does X have no hope of winning in this game?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqdQa_Vrqc-"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1.  X is able to win in the second scenario (3,4,3). It seems as if O is not even trying to prevent X from winning since O already knows that both players play optimally, so X winning is inevitable. This means that any move O selects is equally nonoptimal, so it doesn't matter where O moves.\n",
        "2.  In the last scenario, both players know that all possible optimal plays lead to a draw, so they just move at the first open space until there is a draw and O wins. X has no hope of winning the game because O plays optimally and will always lead the game to a draw/O win."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxOkuptNttkn"
      },
      "source": [
        "In order to expand our agent's capacity to deal with boards larger than (4,4) in a reasonable amount of time, we have to accept some suboptimality. This means cutting off search using depth limiting, which in turn requires an evaluation function for non-terminal states. There is a standard set of strategies for tic-tac-toe which can inform what this looks like, but we'll go with a numerical rather than a rule-based function here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xySgEPSgkzW"
      },
      "source": [
        "def eval(state, k):\n",
        "  X_indices = [i for i,s in np.ndenumerate(state) if s=='X']\n",
        "  O_indices = [i for i,s in np.ndenumerate(state) if s=='O']\n",
        "  blanks = [i for i,s in np.ndenumerate(state) if s=='.']\n",
        "\n",
        "  X_and_blanks = X_indices + blanks\n",
        "  Xset = set(X_indices)\n",
        "  Xbset = set(X_and_blanks)\n",
        "  X_score = 0\n",
        "  for i in X_and_blanks:\n",
        "    for seq in sequences(i, k):\n",
        "      if seq.issubset(Xbset):\n",
        "        ratio = len(seq & Xset)/k\n",
        "        X_score = max(X_score, ratio)\n",
        "\n",
        "  O_and_blanks = O_indices + blanks\n",
        "  Oset = set(O_indices)\n",
        "  Obset = set(O_and_blanks)\n",
        "  O_score = 0\n",
        "  for i in O_and_blanks:\n",
        "    for seq in sequences(i, k):\n",
        "      if seq.issubset(Obset):\n",
        "        ratio = len(seq & Oset)/k\n",
        "        O_score = max(O_score, ratio)\n",
        "  \n",
        "  return X_score - O_score"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPpVTX0L2Qsy"
      },
      "source": [
        "## Response 3 (10 points)\n",
        "\n",
        "Please briefly describe a high-level interpretation of what this evaluation function is computing and returning. What do the ```X_score``` and ```O_score``` variables represent? As a hint, the conditional in each of the inner loops is triggered whenever there is a combination of *k* X's and blanks (or O's and blanks) in a row. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZYfVDAw2t-L"
      },
      "source": [
        "This eval funtion is computing a value that represents how close X is to winning by getting k Xs in a row before O does. The X_score and O_score represent a percentage of how close a completable sequence is to being finished. The total funtion returns the (X_score-O_score) to show if X or O is closer to finishing a sequence. If that number is higher, it is better for X.\n",
        "\n",
        "For example, if k=4 and X has 3 Xs in a row then a blank, it would be close to a win. This evaluation funtion shows that.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRTRQWw025Ai"
      },
      "source": [
        "## Coding 2 (15 points)\n",
        "\n",
        "We will also need some new functions to implement alpha-beta depth-limited search. They will look very similar to the original functions with some minor changes. First, the search function will take a ```max_depth``` parameter. The two value functions will also take this, along with a current ```depth``` parameter. \n",
        "\n",
        "After the terminal check, the value functions should check whether the current depth has hit or exceeded the depth limit. If so, it should return the evaluation of the current state (along with no move as it is treated as a terminal). Complete these functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-jGKtfHWGV"
      },
      "source": [
        "def alpha_beta_depth_search(state, player, k, max_depth):\n",
        "  if player == 'X':\n",
        "    value, move = max_value_depth(state, -float(\"inf\"), float(\"inf\"), k, 1, max_depth)\n",
        "  else:\n",
        "    value, move = min_value_depth(state, -float(\"inf\"), float(\"inf\"), k, 1, max_depth)\n",
        "  return value, move\n",
        "\n",
        "def max_value_depth(state, alpha, beta, k, depth, max_depth):\n",
        "  isTerminal, score = terminal(state, k)\n",
        "  if isTerminal:\n",
        "    return score, None\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # check if we are at the max depth\n",
        "  if depth == max_depth:\n",
        "    score = eval(state, k)\n",
        "    return score, None\n",
        "\n",
        "  move = None\n",
        "  maxEval = -float(\"inf\")\n",
        "\n",
        "  for action in actions(state):\n",
        "    # get new state from action\n",
        "    new_state = result(state, 'X', action)\n",
        "\n",
        "    # get the value and move from that state from the min_value function\n",
        "    score, eval_move = min_value_depth(new_state, alpha, beta, k, depth + 1, max_depth)\n",
        "\n",
        "    # if the score is the biggest yet, it is optimal to follow it, so we save that val and move\n",
        "    if score > maxEval:\n",
        "      maxEval = score\n",
        "      move = action\n",
        "\n",
        "    # we see if alpha is greater than beta to prune\n",
        "    alpha = max(alpha, score)\n",
        "    if beta <= alpha:\n",
        "      break\n",
        "  \n",
        "  return maxEval, move\n",
        "\n",
        "\n",
        "def min_value_depth(state, alpha, beta, k, depth, max_depth):\n",
        "  isTerminal, score = terminal(state, k)\n",
        "  if isTerminal:\n",
        "    return score, None\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # check if we are at the max depth\n",
        "  if depth == max_depth:\n",
        "    score = eval(state, k)\n",
        "    return score, None\n",
        "\n",
        "  move = None\n",
        "  minEval = float(\"inf\")\n",
        "\n",
        "  for action in actions(state):\n",
        "    # get new state from action\n",
        "    new_state = result(state, 'O', action)\n",
        "\n",
        "    # get the value and move from that state from the min_value function\n",
        "    score, eval_move = max_value_depth(new_state, alpha, beta, k, depth + 1, max_depth)\n",
        "\n",
        "    # if the score is the smallest yet, it is optimal to follow it, so we save that val and move\n",
        "    if score < minEval:\n",
        "      minEval = score\n",
        "      move = action\n",
        "\n",
        "    # update beta\n",
        "    beta = min(beta, score)\n",
        "  \n",
        "  return minEval, move\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf5RanFe4cj2"
      },
      "source": [
        "We can now try our hand a larger boards, provided that we sufficiently limit the search depth. Of course, we now lose any guarantee of optimality, and we may get different results when using different depths. We have two games below calling ```alpha_beta_depth_search```. Search depths of 5 are the default max depths, but you'll be trying a few other combinations when answering the following questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUFAqUumTcFd",
        "outputId": "5c0c367d-5070-4b6e-cbd3-ac75f3b6b60b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#m, n, k = 4, 4, 3\n",
        "m, n, k = 5, 5, 4\n",
        "print((m,n,k))\n",
        "initial = np.full((m,n), '.')\n",
        "max_depth_X = 5\n",
        "max_depth_O = 2\n",
        "game_loop(initial, k, alpha_beta_depth_search, [max_depth_X], [max_depth_O])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 5, 4)\n",
            "[['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' '.' 'X' '.' '.']\n",
            " ['O' '.' '.' '.' '.']\n",
            " ['.' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' '.' '.']\n",
            " ['O' '.' '.' '.' '.']\n",
            " ['.' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' '.' '.']\n",
            " ['O' '.' '.' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['.' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' '.' '.']\n",
            " ['O' '.' '.' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' '.' '.']\n",
            " ['O' 'O' '.' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' '.']\n",
            " ['O' 'O' '.' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' '.' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' 'X' '.' '.']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' 'X' '.' 'O']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' 'X' 'X' 'O']\n",
            " ['O' '.' 'X' '.' '.']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' 'X' 'X' 'O']\n",
            " ['O' '.' 'X' '.' 'O']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "[['O' 'X' 'X' 'X' 'O']\n",
            " ['O' 'O' 'X' 'X' 'O']\n",
            " ['O' 'X' 'X' '.' 'O']\n",
            " ['X' '.' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.']] \n",
            "\n",
            "X wins!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n7UTpN562c7"
      },
      "source": [
        "## Response 4 (10 points)\n",
        "\n",
        "1.   Try different max_depths for the (4,4,3) game, ranging from 3 to 6 or 7 (keep both players identical). Explain any differences that you observe in the sequence of moves and/or outcomes. Why does the gameplay change depending on this parameter?\n",
        "2.   You should see that O wins the (5,5,4) game when they both play at depth 5. Can you change the relative depth parameters to get X to win? Briefly explain how the parameter changes end up helping X and changing the outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w5opgfn76Yw"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "1. For smaller max_depths, the players seem to use human-like short term strategies like blocking when a player is about to win. This strings the game out longer and makes move sequences more varied. For larger max_depths, the sequence of moves is short and less varied, since both players probably already know that X is going to win. No matter what the max_depth is, X always wins. \n",
        "The gameplay changes depending on this parameter, because the players are using eval funtions on nonterminal nodes to decide where to move (not optimal) rather than definite terminal states (optimal).\n",
        "2. Yes, if you change the max_depth_X to 5 and max_depth_O to 2 then X will win. This parameter change helps X have more forsight and plan ahead of O, because it allows X to look 5 moves ahead and select a more optimal move each time. Since O can only look 2 moves ahead, the eval funtion must approximate more and the resulting move is less optimal.\n"
      ]
    }
  ]
}